{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "norman-guess",
   "metadata": {},
   "source": [
    "# 导师制名企实训班商业智能方向 004期 Lesson 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-replication",
   "metadata": {},
   "source": [
    "### Thinking 1:机器学习中的监督学习、非监督学习、强化学习有何区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-spain",
   "metadata": {},
   "source": [
    "* 监督学习（Supervised learning）：\n",
    "监督学习即具有特征（feature）和标签（label）的，即使数据是没有标签的，也可以通过学习特征和标签之间的关系，判断出标签——分类。  \n",
    "简言之：提供数据，预测标签。比如对动物猫和狗的图片进行预测，预测label为cat或者dog。  \n",
    "通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如分类。  \n",
    "如分类和回归问题\n",
    "* 无监督学习（Unsupervised learning）：\n",
    "无监督学习即只有特征，没有标签，只有特征，没有标签的训练数据集中，通过数据之间的内在联系和相似性将他们分成若干类——聚类。根据数据本身的特性，从数据中根据某种度量学习出一些特性。  \n",
    "比如一个人没有见过恐龙和鲨鱼，如果给他看了大量的恐龙和鲨鱼，虽然他没有恐龙和鲨鱼的概念，但是他能够观察出每个物种的共性和两个物种间的区别的，并对这两种动物予以区分。  \n",
    "从数据中，寻找隐藏的关系。\n",
    "* 半监督学习（Semi-Supervised learning）：\n",
    "半监督学习使用的数据，一部分是标记过的，而大部分是没有标记的，和监督学习相比较，半监督学习的成本较低，但是又能达到较高的准确度，即综合利用有类标的和没有类标的数据，来生成合适的分类函数。  \n",
    "用少部分标记数据和大部分未知数据进行学习  \n",
    "* 强化学习（Reinforcement learning）：  \n",
    "强化学习与半监督学习类似，均使用未标记的数据，但是强化学习通过算法学习是否距离目标越来越近，通过不断激励与惩罚，达到最终目的。  \n",
    "\n",
    "\n",
    "监督学习有反馈，无监督学习无反馈，强化学习是执行多步之后才反馈。\n",
    "强化学习的目标与监督学习的目标不一样，即强化学习看重的是行为序列下的长期收益，而监督学习往往关注的是和标签或已知输出的误差。  \n",
    "强化学习的奖惩概念是没有正确或错误之分的，而监督学习标签就是正确的，并且强化学习是一个学习+决策的过程，有和环境交互的能力（交互的结果以惩罚的形式返回），而监督学习不具备。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-console",
   "metadata": {},
   "source": [
    "### Thinking 2: 什么是策略网络，价值网络，有何区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-observation",
   "metadata": {},
   "source": [
    "所谓价值网络，是用一个“价值”数来评估当前的棋局。如果我们把棋局上所有棋子的位置总和称为一个“状态”，每个状态可能允许若干不同的后续状态。所有可能状态的前后次序关系就构成了所谓的搜索树。一个暴力的搜索算法会遍历这个搜索树的每一个子树。但是，其实有些状态是较容易判断输赢的，也就是评估其“价值”。我们把这些状态用价值表示，就可以据此省略了对它所有后续状态的探索，即利用价值网络削减搜索深度。  \n",
    "所谓策略，是指在给定棋局，评估每一种应对可能的胜率，从而根据当前盘面状态来选择走棋策略。在数学上，就是估计一个在各个合法位置上下子获胜的可能的概率分布。因为有些下法的获胜概率很低，可忽略，所以用策略评估就可以消减搜索树的宽度。   \n",
    "所谓“价值”就是能看懂棋局，一眼就能判断某给定棋局是不是能赢，这是个偏宏观的评估。所谓的“策略”，是指在每一步博弈时，各种选择的取舍，这是个偏微观的评估。AlphaGo利用模拟棋手、强化自我的方法，在宏观（价值评估）和微观（策略评估）两个方面提高了探索的效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-eleven",
   "metadata": {},
   "source": [
    "### Thinking 3: 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-membrane",
   "metadata": {},
   "source": [
    "MCTS蒙特卡洛树搜索原理，蒙特卡洛通过多次模拟仿真，预测出最佳策略。搜索是对整棵博弈树的组合遍历，单次的遍历是从根结点开始，到一个未完全展开的节点(a node that is not full expanded)。未完全展开的意思就是它至少有一个孩子节点未被访问，或者称作未被探索过。当遇到未被完全展开过的节点，选择它的一个未被访问的childre node做根结点，进行一次模拟(a single playout/simulation)。仿真的结果反向传播(propagated back)用于更新当前树的根结点,并更新博弈树节点的统计信息。当整棵博弈树搜索结束后，就相当于拿到了这颗博弈树的策略。  \n",
    "分为四个步骤：\n",
    "1. 选择（Select）：从根节点开始，按一定策略，搜索到叶子节点。\n",
    "2. 扩展（Expansion）：对叶子节点扩展一个或多个合法的子节点。\n",
    "3. 模拟（Simluation）：对子节点采用随机的（即称为蒙特卡洛的缘由）方式模拟若干次实验。模拟到最终状态时即可得到当前模拟器所得的分。\n",
    "4. 结果回传（Backpropagation）：根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-bundle",
   "metadata": {},
   "source": [
    "### Thinking4: 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-champagne",
   "metadata": {},
   "source": [
    "强化学习可以通过与用户不断交互的过程中学习用户兴趣，给用户推荐用户需要的信息。使用强化学习需要考虑如何设置reward，要综合考虑用户看视频的时长、点赞、评论等等反馈。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-moral",
   "metadata": {},
   "source": [
    "### Thinking5: 在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-identification",
   "metadata": {},
   "source": [
    "在自动驾驶中，使用摄像头等传感器接收的数据作为输入的状态特征，前进、后退、左转右转等作为动作，正常运行的时长等作为反馈。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-evening",
   "metadata": {},
   "source": [
    "### Action \n",
    "Action（五子棋）：  \n",
    "棋盘大小 10 * 10  \n",
    "采用强化学习（策略价值网络），用AI训练五子棋AI  \n",
    "请说明都有哪些模块，不同模块的原理  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-mirror",
   "metadata": {},
   "source": [
    "#### 查看GPU情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "positive-infrared",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "GeForce GTX 1060\n",
      "tensor([[0.0177, 0.1537, 0.0022],\n",
      "        [0.9854, 0.2411, 0.7439],\n",
      "        [0.3781, 0.2301, 0.6413]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "flag = torch.cuda.is_available()\n",
    "print(flag)\n",
    "\n",
    "ngpu= 1\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.rand(3,3).cuda()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-entry",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sudden-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TrainPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-quick",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\LearnBI_Code\\VIP_Class\\Homeworks\\20210228_L22\\policy_value_net_pytorch.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_act = F.log_softmax(self.act_fc1(x_act))\n",
      "D:\\ProgramData\\Anaconda\\envs\\learnBI\\lib\\site-packages\\torch\\nn\\functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch i:1, episode_len:15\n",
      "batch i:2, episode_len:22\n",
      "batch i:3, episode_len:20\n",
      "batch i:4, episode_len:24\n",
      "kl:0.03014,lr_multiplier:1.000,loss:5.359708786010742,entropy:4.589460372924805,explained_var_old:-0.002,explained_var_new:0.388\n",
      "batch i:5, episode_len:13\n",
      "kl:0.01520,lr_multiplier:1.000,loss:5.435701370239258,entropy:4.565437316894531,explained_var_old:0.169,explained_var_new:0.112\n",
      "batch i:6, episode_len:17\n",
      "kl:0.01818,lr_multiplier:1.000,loss:5.523181915283203,entropy:4.529996395111084,explained_var_old:0.038,explained_var_new:0.040\n",
      "batch i:7, episode_len:23\n",
      "kl:0.01864,lr_multiplier:1.000,loss:5.561110019683838,entropy:4.563239574432373,explained_var_old:-0.028,explained_var_new:-0.007\n",
      "batch i:8, episode_len:21\n",
      "kl:0.00783,lr_multiplier:1.500,loss:5.551912307739258,entropy:4.550752639770508,explained_var_old:-0.030,explained_var_new:0.002\n",
      "batch i:9, episode_len:22\n",
      "kl:0.00525,lr_multiplier:2.250,loss:5.5410990715026855,entropy:4.548686981201172,explained_var_old:0.001,explained_var_new:0.001\n",
      "batch i:10, episode_len:14\n",
      "kl:0.00688,lr_multiplier:3.375,loss:5.525965213775635,entropy:4.540151596069336,explained_var_old:0.001,explained_var_new:0.007\n",
      "batch i:11, episode_len:15\n",
      "kl:0.03077,lr_multiplier:3.375,loss:5.476284027099609,entropy:4.476346969604492,explained_var_old:0.004,explained_var_new:0.021\n",
      "batch i:12, episode_len:31\n",
      "kl:0.15511,lr_multiplier:2.250,loss:5.450840473175049,entropy:4.46235466003418,explained_var_old:0.002,explained_var_new:0.015\n",
      "batch i:13, episode_len:15\n",
      "kl:0.12328,lr_multiplier:1.500,loss:5.536041736602783,entropy:4.216137409210205,explained_var_old:0.003,explained_var_new:0.013\n",
      "batch i:14, episode_len:16\n",
      "kl:0.03994,lr_multiplier:1.500,loss:5.462985515594482,entropy:4.48832368850708,explained_var_old:0.011,explained_var_new:0.029\n",
      "batch i:15, episode_len:19\n",
      "kl:0.03790,lr_multiplier:1.500,loss:5.384239196777344,entropy:4.4444966316223145,explained_var_old:0.038,explained_var_new:0.052\n",
      "batch i:16, episode_len:30\n",
      "kl:0.05331,lr_multiplier:1.000,loss:5.234461784362793,entropy:4.295852184295654,explained_var_old:0.081,explained_var_new:0.130\n",
      "batch i:17, episode_len:25\n",
      "kl:0.05149,lr_multiplier:0.667,loss:5.330328941345215,entropy:4.462400436401367,explained_var_old:0.045,explained_var_new:0.063\n",
      "batch i:18, episode_len:20\n",
      "kl:0.01689,lr_multiplier:0.667,loss:5.355787754058838,entropy:4.4108734130859375,explained_var_old:0.045,explained_var_new:0.054\n",
      "batch i:19, episode_len:24\n",
      "kl:0.00413,lr_multiplier:1.000,loss:5.345677852630615,entropy:4.38381290435791,explained_var_old:0.046,explained_var_new:0.063\n",
      "batch i:20, episode_len:9\n",
      "kl:0.01274,lr_multiplier:1.000,loss:5.258045196533203,entropy:4.3809380531311035,explained_var_old:0.081,explained_var_new:0.105\n",
      "batch i:21, episode_len:17\n",
      "kl:0.01562,lr_multiplier:1.000,loss:5.331732749938965,entropy:4.4486823081970215,explained_var_old:0.066,explained_var_new:0.088\n",
      "batch i:22, episode_len:31\n",
      "kl:0.05657,lr_multiplier:0.667,loss:5.2581048011779785,entropy:4.3172454833984375,explained_var_old:0.069,explained_var_new:0.101\n",
      "batch i:23, episode_len:21\n",
      "kl:0.03022,lr_multiplier:0.667,loss:5.322147369384766,entropy:4.43461799621582,explained_var_old:0.081,explained_var_new:0.101\n",
      "batch i:24, episode_len:34\n",
      "kl:0.01669,lr_multiplier:0.667,loss:5.2048234939575195,entropy:4.327272415161133,explained_var_old:0.089,explained_var_new:0.117\n",
      "batch i:25, episode_len:31\n",
      "kl:0.00814,lr_multiplier:1.000,loss:5.224631309509277,entropy:4.308681488037109,explained_var_old:0.079,explained_var_new:0.111\n",
      "batch i:26, episode_len:11\n",
      "kl:0.01420,lr_multiplier:1.000,loss:5.246838569641113,entropy:4.415642738342285,explained_var_old:0.081,explained_var_new:0.119\n",
      "batch i:27, episode_len:20\n",
      "kl:0.02439,lr_multiplier:1.000,loss:5.103009223937988,entropy:4.205451011657715,explained_var_old:0.123,explained_var_new:0.181\n",
      "batch i:28, episode_len:14\n",
      "kl:0.02846,lr_multiplier:1.000,loss:5.129790306091309,entropy:4.369375228881836,explained_var_old:0.164,explained_var_new:0.200\n",
      "batch i:29, episode_len:11\n",
      "kl:0.04524,lr_multiplier:0.667,loss:5.092531204223633,entropy:4.190464973449707,explained_var_old:0.132,explained_var_new:0.225\n",
      "batch i:30, episode_len:26\n",
      "kl:0.03686,lr_multiplier:0.667,loss:4.889038562774658,entropy:4.274369716644287,explained_var_old:0.267,explained_var_new:0.329\n",
      "batch i:31, episode_len:30\n",
      "kl:0.01773,lr_multiplier:0.667,loss:4.977685928344727,entropy:4.240396499633789,explained_var_old:0.249,explained_var_new:0.290\n",
      "batch i:32, episode_len:27\n",
      "kl:0.02515,lr_multiplier:0.667,loss:4.8989996910095215,entropy:4.216874122619629,explained_var_old:0.220,explained_var_new:0.322\n",
      "batch i:33, episode_len:10\n",
      "kl:0.03820,lr_multiplier:0.667,loss:4.862555503845215,entropy:4.17707633972168,explained_var_old:0.282,explained_var_new:0.347\n",
      "batch i:34, episode_len:20\n",
      "kl:0.02411,lr_multiplier:0.667,loss:4.826286315917969,entropy:4.210493087768555,explained_var_old:0.279,explained_var_new:0.343\n",
      "batch i:35, episode_len:23\n",
      "kl:0.01385,lr_multiplier:0.667,loss:4.879208087921143,entropy:4.188523769378662,explained_var_old:0.252,explained_var_new:0.323\n",
      "batch i:36, episode_len:13\n",
      "kl:0.01905,lr_multiplier:0.667,loss:4.78724479675293,entropy:4.164646148681641,explained_var_old:0.303,explained_var_new:0.391\n",
      "batch i:37, episode_len:14\n",
      "kl:0.02021,lr_multiplier:0.667,loss:4.794671058654785,entropy:4.195820331573486,explained_var_old:0.326,explained_var_new:0.395\n",
      "batch i:38, episode_len:7\n",
      "kl:0.03271,lr_multiplier:0.667,loss:4.694620132446289,entropy:4.108756065368652,explained_var_old:0.333,explained_var_new:0.451\n",
      "batch i:39, episode_len:20\n",
      "kl:0.03901,lr_multiplier:0.667,loss:4.716124534606934,entropy:4.197425365447998,explained_var_old:0.340,explained_var_new:0.429\n",
      "batch i:40, episode_len:19\n",
      "kl:0.04973,lr_multiplier:0.444,loss:4.621941566467285,entropy:3.997356653213501,explained_var_old:0.362,explained_var_new:0.468\n",
      "batch i:41, episode_len:18\n",
      "kl:0.02041,lr_multiplier:0.444,loss:4.716396331787109,entropy:4.117912292480469,explained_var_old:0.289,explained_var_new:0.352\n",
      "batch i:42, episode_len:28\n",
      "kl:0.00872,lr_multiplier:0.667,loss:4.93352746963501,entropy:4.189244270324707,explained_var_old:0.175,explained_var_new:0.256\n",
      "batch i:43, episode_len:20\n",
      "kl:0.01074,lr_multiplier:0.667,loss:4.738297462463379,entropy:4.096653938293457,explained_var_old:0.287,explained_var_new:0.360\n",
      "batch i:44, episode_len:18\n",
      "kl:0.02055,lr_multiplier:0.667,loss:4.6441650390625,entropy:4.030937194824219,explained_var_old:0.279,explained_var_new:0.395\n",
      "batch i:45, episode_len:12\n",
      "kl:0.02046,lr_multiplier:0.667,loss:4.764726638793945,entropy:4.100988864898682,explained_var_old:0.208,explained_var_new:0.316\n"
     ]
    }
   ],
   "source": [
    "training_pipeline = TrainPipeline()\n",
    "training_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-easter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnBI",
   "language": "python",
   "name": "learnbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
